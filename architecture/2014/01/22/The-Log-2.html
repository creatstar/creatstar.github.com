<!DOCTYPE html>
<html>
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <title>The Log: What every software engineer should know about real-time data's unifying abstraction (2)</title>
        <meta name="viewport" content="width=device-width">

        <!-- syntax highlighting CSS -->
        <link rel="stylesheet" href="/css/syntax.css">

        <!-- Custom CSS -->
        <link rel="stylesheet" href="/css/main.css">
        <link rel="stylesheet" href="/css/bootstrap.css">
        <link rel="shortcut icon" href="/favicon.ico"> 
        <script>
          (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
          })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
        
          ga('create', 'UA-46851282-1', 'creatstar.com');
          ga('send', 'pageview');
        
        </script>
    </head>
    <body>

        <div class="site">
          <div class="header">
            <h1 class="title"><a href="/">INDOMITABLE HERMIT</a></h1>
            <a class="extra" href="/">home</a>
          </div>

          <h2>The Log: What every software engineer should know about real-time data's unifying abstraction (2)</h2>
<p class="meta">22 Jan 2014</p>

<div class="post">
<p>上一部分链接:<a href="/architecture/2014/01/18/The-Log-1.html">The Log: What every software engineer should know about real-time data&#39;s unifying abstraction (1)</a></p>

<h3>Part Two: Data Integration</h3>

<p>Let me first say what I mean by &quot;data integration&quot; and why I think it&#39;s important, then we&#39;ll see how it relates back to logs.</p>

<ul>
<li>Data integration is making all the data an organization has available in all its services and systems.</li>
</ul>

<p>This phrase &quot;data integration&quot; isn&#39;t all that common, but I don&#39;t know a better one. The more recognizable term ETL usually covers only a limited part of data integration—populating a relational data warehouse. But much of what I am describing can be thought of as ETL generalized to cover real-time systems and processing flows.</p>

<blockquote>
<p>ETL通常只包含Data integration的部分，即，数据进入一个关系型数据仓库。但作者所说的Data integration概念指的是ETL概念泛化到实时系统和数据处理流。</p>
</blockquote>

<p>You don&#39;t hear much about data integration in all the breathless interest and hype around the idea of big data, but nonetheless, I believe this mundane problem of &quot;making the data available&quot; is one of the more valuable things an organization can focus on.</p>

<p>Effective use of data follows a kind of Maslow&#39;s hierarchy of needs. The base of the pyramid involves capturing all the relevant data, being able to put it together in an applicable processing environment (be that a fancy real-time query system or just text files and python scripts). This data needs to be modeled in a uniform way to make it easy to read and process. Once these basic needs of capturing data in a uniform way are taken care of it is reasonable to work on infrastructure to process this data in various ways—MapReduce, real-time query systems, etc.</p>

<p>It&#39;s worth noting the obvious: without a reliable and complete data flow, a Hadoop cluster is little more than a very expensive and difficult to assemble space heater. Once data and processing are available, one can move concern on to more refined problems of good data models and consistent well understood semantics. Finally, concentration can shift to more sophisticated processing—better visualization, reporting, and algorithmic processing and prediction.</p>

<blockquote>
<p>类比马斯洛的需求理论，把所有的相关数据都放在一起，并统一它们的格式以便读取和处理，是有效利用数据的最基础的需求。如果有稳定和完整的数据流，我们就可以把注意力转移到更加精细的可视化技术，报表，以及数值处理和预测上去。</p>
</blockquote>

<p>In my experience, most organizations have huge holes in the base of this pyramid—they lack reliable complete data flow—but want to jump directly to advanced data modeling techniques. This is completely backwards.</p>

<p>So the question is, how can we build reliable data flow throughout all the data systems in an organization?</p>

<h4>Data Integration: Two complications</h4>

<p>Two trends make data integration harder.</p>

<blockquote>
<p>两个趋势使得data integration变得复杂</p>
</blockquote>

<h5>The event data firehose</h5>

<p>The first trend is the rise of event data. Event data records things that happen rather than things that are. In web systems, this means user activity logging, but also the machine-level events and statistics required to reliably operate and monitor a data center&#39;s worth of machines. People tend to call this &quot;log data&quot; since it is often written to application logs, but that confuses form with function. This data is at the heart of the modern web: Google&#39;s fortune, after all, is generated by a relevance pipeline built on clicks and impressions—that is, events.</p>

<p>And this stuff isn&#39;t limited to web companies, it&#39;s just that web companies are already fully digital, so they are easier to instrument. Financial data has long been event-centric. RFID adds this kind of tracking to physical objects. I think this trend will continue with the digitization of traditional businesses and activities.</p>

<p>This type of event data records what happened, and tends to be several orders of magnitude larger than traditional database uses. This presents significant challenges for processing.</p>

<blockquote>
<p>事件数据的快速增长。由于在事件数据中不仅保存了用户的行为数据，还保存了硬件事件和数据，导致数据膨胀很快。而且这种趋势不仅仅局限于互联网公司。这样的数据规模超出传统数据库处理能力的几个数量级。事件数据膨胀带来的问题本人也深有体会。对于具体的用户来说，事件数据的自定义比较方便，容易加入各种需求。但对于data integration来说，数据自定义带来的格式多样性极大地降低了数据的价值，提高了数据利用的难度。所以需要在设计之初就考虑事件数据的格式化和模版化。</p>
</blockquote>

<h5>The explosion of specialized data systems</h5>

<p>The second trend comes from the explosion of specialized data systems that have become popular and often freely available in the last five years. Specialized systems exist for OLAP, search, simple online storage, batch processing, graph analysis, and so on.</p>

<p>The combination of more data of more varieties and a desire to get this data into more systems leads to a huge data integration problem.</p>

<blockquote>
<p>特殊数据系统的爆炸式增长也是一个重要的趋势，这些特殊数据系统包括 OLAP，搜索，简单在线存储，批量处理，图分析等。</p>
</blockquote>

<h4>Log-structured data flow</h4>

<p>The log is the natural data structure for handling data flow between systems. The recipe is very simple:</p>

<ul>
<li>Take all the organization&#39;s data and put it into a central log for real-time subscription.
&gt;日志结构组织的数据流的核心就是把组织中所有数据都放到一个中心log系统里，以便实时的订阅。</li>
</ul>

<p>Each logical data source can be modeled as its own log. A data source could be an application that logs out events (say clicks or page views), or a database table that accepts modifications. Each subscribing system reads from this log as quickly as it can, applies each new record to its own store, and advances its position in the log. Subscribers could be any kind of data system—a cache, Hadoop, another database in another site, a search system, etc.</p>

<p>For example, the log concept gives a logical clock for each change against which all subscribers can be measured. This makes reasoning about the state of the different subscriber systems with respect to one another far simpler, as each has a &quot;point in time&quot; they have read up to.</p>

<p><img src="/generated/2014-01-22/resource-299x285-bf053c.png" alt="log-structed data flow" ></p>

<p>To make this more concrete, consider a simple case where there is a database and a collection of caching servers. The log provides a way to synchronize the updates to all these systems and reason about the point of time of each of these systems. Let&#39;s say we write a record with log entry X and then need to do a read from the cache. If we want to guarantee we don&#39;t see stale data, we just need to ensure we don&#39;t read from any cache which has not replicated up to X.</p>

<blockquote>
<p>日志的概念实际上给订阅者标识进度提供了一个逻辑时钟。这个逻辑时钟对于应用层来说也是重要的，可以用于避免某个订阅系统读到过时的数据。</p>
</blockquote>

<p>The log also acts as a buffer that makes data production asynchronous from data consumption. This is important for a lot of reasons, but particularly when there are multiple subscribers that may consume at different rates. This means a subscribing system can crash or go down for maintenance and catch up when it comes back: the subscriber consumes at a pace it controls. A batch system such as Hadoop or a data warehouse may consume only hourly or daily, whereas a real-time query system may need to be up-to-the-second. Neither the originating data source nor the log has knowledge of the various data destination systems, so consumer systems can be added and removed with no change in the pipeline.</p>

<blockquote>
<p>日志也表现为数据异步消费时的缓冲，因为每个订阅者的消费速度不一样，订阅系统可能停机、维护，在重新连接时追上进度。像data warehouse或者hadoop这样的批量系统，又有可能以每小时或者每天的间隔消费数据，而实时系统却需要实时消费。数据源不可能知道所有目标系统的细节，所以消费者的增加和删除都不应该对pipeline有影响。</p>
</blockquote>

<p>Of particular importance: the destination system only knows about the log and not any details of the system of origin. The consumer system need not concern itself with whether the data came from an RDBMS, a new-fangled key-value store, or was generated without a real-time query system of any kind. This seems like a minor point, but is in fact critical.</p>

<blockquote>
<p>很重要的一点是，消费者在从系统中取出数据的时候，并不需要关心数据的来源，这样就使得在整个数据处理pipeline中加入和减少环节变得更加容易。</p>
</blockquote>

<p>I use the term &quot;log&quot; here instead of &quot;messaging system&quot; or &quot;pub sub&quot; because it is a lot more specific about semantics and a much closer description of what you need in a practical implementation to support data replication. I have found that &quot;publish subscribe&quot; doesn&#39;t imply much more than indirect addressing of messages—if you compare any two messaging systems promising publish-subscribe, you find that they guarantee very different things, and most models are not useful in this domain. You can think of the log as acting as a kind of messaging system with durability guarantees and strong ordering semantics. In distributed systems, this model of communication sometimes goes by the (somewhat terrible) name of atomic broadcast.</p>

<p>It&#39;s worth emphasizing that the log is still just the infrastructure. That isn&#39;t the end of the story of mastering data flow: the rest of the story is around metadata, schemas, compatibility, and all the details of handling data structure and evolution. But until there is a reliable, general way of handling the mechanics of data flow, the semantic details are secondary.</p>

<blockquote>
<p>值得再次强调的是，log仍然只是基础架构，它并不是操控数据流的最后一环，剩下的事情还包括metadata，schemas，兼容性，还有处理数据结构和演化的各种细节。当然，在拥有一个可靠和统一处理数据流的机制之前，语义的细节是第二位的。</p>
</blockquote>

<h4>At LinkedIn</h4>

<p>I got to watch this data integration problem emerge in fast-forward as LinkedIn moved from a centralized relational database to a collection of distributed systems.
These days our major data systems include:</p>

<ul>
<li><a href="http://data.linkedin.com/projects/search">Search</a></li>
<li><a href="http://engineering.linkedin.com/real-time-distributed-graph/using-set-cover-algorithm-optimize-query-latency-large-scale-distributed">Social Graph</a></li>
<li><a href="http://project-voldemort.com/">Voldemort</a> (key-value store)</li>
<li><a href="http://data.linkedin.com/projects/espresso">Espresso</a> (document store)</li>
<li><a href="http://www.quora.com/LinkedIn-Recommendations/How-does-LinkedIns-recommendation-system-work">Recommendation engine</a></li>
<li>OLAP query engine</li>
<li><a href="http://hadoop.apache.org/">Hadoop</a></li>
<li><a href="http://www.teradata.com/">Terradata</a></li>
<li><a href="http://engineering.linkedin.com/52/autometrics-self-service-metrics-collection">Ingraphs</a> (monitoring graphs and metrics services)</li>
</ul>

<p>Each of these is a specialized distributed system that provides advanced functionality in its area of specialty.</p>

<p>This idea of using logs for data flow has been floating around LinkedIn since even before I got here. One of the earliest pieces of infrastructure we developed was a service called <a href="https://github.com/linkedin/databus">databus</a> that provided a log caching abstraction on top of our early Oracle tables to scale subscription to database changes so we could feed our social graph and search indexes.</p>

<p>I&#39;ll give a little bit of the history to provide context. My own involvement in this started around 2008 after we had shipped our key-value store. My next project was to try to get a working Hadoop setup going, and move some of our recommendation processes there. Having little experience in this area, we naturally budgeted a few weeks for getting data in and out, and the rest of our time for implementing fancy prediction algorithms. So began a long slog.</p>

<p>We originally planned to just scrape the data out of our existing Oracle data warehouse. The first discovery was that getting data out of Oracle quickly is something of a dark art. Worse, the data warehouse processing was not appropriate for the production batch processing we planned for Hadoop—much of the processing was non-reversable and specific to the reporting being done. We ended up avoiding the data warehouse and going directly to source databases and log files. Finally, we implemented another pipeline to load data into our key-value store for serving results.</p>

<blockquote>
<p>本文作者从Oracle数据仓库导数据到hadoop时出现问题，最后直接从原始数据库和日志文件导数据。并且实现了一个向kv存储（用于线上展示数据）加载数据的pipeline</p>
</blockquote>

<p>This mundane data copying ended up being one of the dominate items for the original development. Worse, any time there was a problem in any of the pipelines, the Hadoop system was largely useless—running fancy algorithms on bad data just produces more bad data.</p>

<blockquote>
<p>hadoop在坏数据上跑，得到的也是坏数据。</p>
</blockquote>

<p>Although we had built things in a fairly generic way, each new data source required custom configuration to set up. It also proved to be the source of a huge number of errors and failures. The site features we had implemented on Hadoop became popular and we found ourselves with a long list of interested engineers. Each user had a list of systems they wanted integration with and a long list of new data feeds they wanted.</p>

<blockquote>
<p>虽然构建了一个较为通用的运行方式，但每个新数据源的加入都需要自定义的配置。这个过程常常会带来不少错误和失败。</p>
</blockquote>

<p>A few things slowly became clear to me.</p>

<p>First, the pipelines we had built, though a bit of a mess, were actually extremely valuable. Just the process of making data available in a new processing system (Hadoop) unlocked a lot of possibilities. New computation was possible on the data that would have been hard to do before. Many new products and analysis just came from putting together multiple pieces of data that had previously been locked up in specialized systems.</p>

<blockquote>
<p>将数据放到一个新的处理系统里，是一件很有价值的事情，很多的新产品和分析都是通过把多种数据整合到一起而产生的</p>
</blockquote>

<p>Second, it was clear that reliable data loads would require deep support from the data pipeline. If we captured all the structure we needed, we could make Hadoop data loads fully automatic, so that no manual effort was expanded adding new data sources or handling schema changes—data would just magically appear in HDFS and Hive tables would automatically be generated for new data sources with the appropriate columns.</p>

<blockquote>
<p>可靠的数据读入依赖于数据管道的支持，如果有合适的数据格式，将数据导入特定系统的工作可以是全自动化的。</p>
</blockquote>

<p>Third, we still had very low data coverage. That is, if you looked at the overall percentage of the data LinkedIn had that was available in Hadoop, it was still very incomplete. And getting to completion was not going to be easy given the amount of effort required to operationalize each new data source.</p>

<blockquote>
<p>LinkedIn的可供分析的数据覆盖率仍然很低。</p>
</blockquote>

<p>The way we had been proceeding, building out custom data loads for each data source and destination, was clearly infeasible. We had dozens of data systems and data repositories. Connecting all of these would have lead to building custom piping between each pair of systems something like this:</p>

<p><img src="/generated/2014-01-22/mess_dataflow-598x206-0a7454.png" alt="mess_dataflow.png" ></p>

<p>Note that data often flows in both directions, as many systems (databases, Hadoop) are both sources and destinations for data transfer. This meant we would end up building two pipelines per system: one to get data in and one to get data out.</p>

<blockquote>
<p>在实际生产环境中，任何系统都可能同时既是数据源又是数据出口。如果在任意两个系统间都有两个管道（出口和入口），那整个环境的复杂性是非常可怕的。</p>
</blockquote>

<p>This clearly would take an army of people to build and would never be operable. As we approached fully connectivity we would end up with something like O(N2) pipelines.</p>

<p>Instead, we needed something generic like this:</p>

<p><img src="/generated/2014-01-22/clean_dataflow-598x251-56fe8e.png" alt="mess_dataflow.png" ></p>

<p>As much as possible, we needed to isolate each consumer from the source of the data. They should ideally integrate with just a single data repository that would give them access to everything.</p>

<p>The idea is that adding a new data system—be it a data source or a data destination—should create integration work only to connect it to a single pipeline instead of each consumer of data.</p>

<blockquote>
<p>为了解决这个问题，我们需要尽可能做到把数据的消费从数据源处隔离开。统一的管道而不是各个独立的管道是一个比较自然的想法。</p>
</blockquote>

<p>This experience lead me to focus on building Kafka to combine what we had seen in messaging systems with the log concept popular in databases and distributed system internals. We wanted something to act as a central pipeline first for all activity data, and eventually for many other uses, including data deployment out of Hadoop, monitoring data, etc.</p>

<p>For a long time, Kafka was a little unique (some would say odd) as an infrastructure product—neither a database nor a log file collection system nor a traditional messaging system. But recently Amazon has offered a service that is very very similar to Kafka called Kinesis. The similarity goes right down to the way partitioning is handled, data is retained, and the fairly odd split in the Kafka API between high- and low-level consumers. I was pretty happy about this. A sign you&#39;ve created a good infrastructure abstraction is that AWS offers it as a service! Their vision for this seems to be exactly similar to what I am describing: it is the piping that connects all their distributed systems—DynamoDB, RedShift, S3, etc.—as well as the basis for distributed stream processing using EC2.</p>

<blockquote>
<p>亚马逊也推出了一个类似kafka的服务，Kinesis</p>
</blockquote>

<h4>Relationship to ETL and the Data Warehouse</h4>

<p>Let&#39;s talk data warehousing for a bit. The data warehouse is meant to be a repository of the clean, integrated data structured to support analysis. This is a great idea. For those not in the know, the data warehousing methodology involves periodically extracting data from source databases, munging it into some kind of understandable form, and loading it into a central data warehouse. Having this central location that contains a clean copy of all your data is a hugely valuable asset for data-intensive analysis and processing. At a high level, this methodology doesn&#39;t change too much whether you use a traditional data warehouse like Oracle or Teradata or Hadoop, though you might switch up the order of loading and munging.</p>

<blockquote>
<p>数据仓库的应用方式是基本一致的：周期性地把数据从源数据库中抽取出来，变换成某种可以理解的格式，读入中央数据仓库。当中央数据仓库保有一个干净数据副本时，它就能用来进行数据密集型分析和处理。</p>
</blockquote>

<p>A data warehouse containing clean, integrated data is a phenomenal asset, but the mechanics of getting this are a bit out of date.</p>

<p>The key problem for a data-centric organization is coupling the clean integrated data to the data warehouse. A data warehouse is a piece of batch query infrastructure which is well suited to many kinds of reporting and ad hoc analysis, particularly when the queries involve simple counting, aggregation, and filtering. But having a batch system be the only repository of clean complete data means the data is unavailable for systems requiring a real-time feed—real-time processing, search indexing, monitoring systems, etc.</p>

<blockquote>
<p>以数据为中心组织系统的关键点在于把干净的数据整合入数据仓库中。数据仓库是批量查询基础架构的一部分，它很适合于多种类的报表和特殊分析系统，特别是查询包含简单的计数，累加，和过滤时。但批量系统以为着这些干净完整的数据无法被实时或准实时的处理，搜索索引，以及监控系统所使用。</p>
</blockquote>

<p>In my view, ETL is really two things. First, it is an extraction and data cleanup process—essentially liberating data locked up in a variety of systems in the organization and removing an system-specific non-sense. Secondly, that data is restructured for data warehousing queries (i.e. made to fit the type system of a relational DB, forced into a star or snowflake schema, perhaps broken up into a high performance column format, etc). Conflating these two things is a problem. The clean, integrated repository of data should be available in real-time as well for low-latency processing as well as indexing in other real-time storage systems.</p>

<blockquote>
<p>在本文作者眼中，ETL包含了两件事情：1. 它是一个抽取和清洗过程，在去掉一些无意义和特殊系统相关的信息后可以极大地在各个系统间分享数据。2. 使得的数据被结构化成适合数据仓库查询的格式。把这两件事情合并起来是一个难题。干净和完整的数据仓储应该开放给实时，低延时的处理（包括在其他实时存储系统中的索引）。</p>
</blockquote>

<p>I think this has the added benefit of making data warehousing ETL much more organizationally scalable. The classic problem of the data warehouse team is that they are responsible for collecting and cleaning all the data generated by every other team in the organization. The incentives are not aligned: data producers are often not very aware of the use of the data in the data warehouse and end up creating data that is hard to extract or requires heavy, hard to scale transformation to get into usable form. Of course, the central team never quite manages to scale to match the pace of the rest of the organization, so data coverage is always spotty, data flow is fragile, and changes are slow.</p>

<blockquote>
<p>构建一个更加可扩展的数据仓库ETL系统是很有意义的。在传统情况下，数据仓库团队负责所有其他team产生数据的收集和清理工作，由于目标不一致，数据生产方不理解数据在数据仓库中的使用方式，常常会生产出难以抽取，或者需要大量资源来转换的数据格式，这样数据仓库team就很难给出可扩展方案，所以组织内部的数据使用覆盖率很低，数据流很脆弱，数据更新也很慢。</p>
</blockquote>

<p>A better approach is to have a central pipeline, the log, with a well defined API for adding data. The responsibility of integrating with this pipeline and providing a clean, well-structured data feed lies with the producer of this data feed. This means that as part of their system design and implementation they must consider the problem of getting data out and into a well structured form for delivery to the central pipeline. The addition of new storage systems is of no consequence to the data warehouse team as they have a central point of integration. The data warehouse team handles only the simpler problem of loading structured feeds of data from the central log and carrying out transformation specific to their system.</p>

<blockquote>
<p>一个更好的方式是使用一个中央管道，数据源系统在设计的时候就应该考虑能输出干净的，格式化的数据到中央数据管道中。新的系统不用受数据仓库team的影响，因为他们有一个中心点去集成数据。这样数据仓库组只需要简单地从中央日志中获取结构化的数据流，然后做特定的数据转换就可以了。</p>
</blockquote>

<p>This point about organizational scalability becomes particularly important when one considers adopting additional data systems beyond a traditional data warehouse. Say, for example, that one wishes to provide search capabilities over the complete data set of the organization. Or, say that one wants to provide sub-second monitoring of data streams with real-time trend graphs and alerting. In either of these cases, the infrastructure of the traditional data warehouse or even a Hadoop cluster is going to be inappropriate. Worse, the ETL processing pipeline built to support database loads is likely of no use for feeding these other systems, making bootstrapping these pieces of infrastructure as large an undertaking as adopting a data warehouse. This likely isn&#39;t feasible and probably helps explain why most organizations do not have these capabilities easily available for all their data. By contrast, if the organization had built out feeds of uniform, well-structured data, getting any new system full access to all data requires only a single bit of integration plumbing to attach to the pipeline.</p>

<blockquote>
<p>如果需要提供更加深入的功能，更加省时的结果，那么统一格式化能帮助新系统以很小的代价接入中央数据管道。</p>
</blockquote>

<p>This architecture also raises a set of different options for where a particular cleanup or transformation can reside:</p>

<ol>
<li>It can be done by the data producer prior to adding the data to the company wide log. <em>在数据传入log之前就进行clean up和转换。</em></li>
<li>It can be done as a real-time transformation on the log (which in turn produces a new, transformed log) <em>在log管道中实时进行。</em></li>
<li>It can be done as part of the load process into some destination data system <em>在数据读入目标系统时进行clean up和转换。</em></li>
</ol>

<p>The best model is to have cleanup done prior to publishing the data to the log by the publisher of the data. This means ensuring the data is in a canonical form and doesn&#39;t retain any hold-overs from the particular code that produced it or the storage system in which it may have been maintained. These details are best handled by the team that creates the data since they know the most about their own data. Any logic applied in this stage should be lossless and reversible.</p>

<blockquote>
<p>最好的方式是在数据进入log之前就做好clean up，因为数据的产出方总是最了解数据。</p>
</blockquote>

<p>Any kind of value-added transformation that can be done in real-time should be done as post-processing on the raw log feed produced. This would include things like sessionization of event data, or the addition of other derived fields that are of general interest. The original log is still available, but this real-time processing produces a derived log containing augmented data.</p>

<blockquote>
<p>能够实时进行添加值的任务应该作为原始日志的后处理添加。</p>
</blockquote>

<p>Finally, only aggregation that is specific to the destination system should be performed as part of the loading process. This might include transforming data into a particular star or snowflake schema for analysis and reporting in a data warehouse. Because this stage, which most naturally maps to the traditional ETL process, is now done on a far cleaner and more uniform set of streams, it should be much simplified.</p>

<blockquote>
<p>只有对于目标系统的特定聚合操作才应该作为读取过程的一部分。这可能包括为了在数据仓库中分析或者报表将数据变形成为一个特定的星型或者雪花型的schema。这部分和原始ETL最相关的过程现在只需要做非常简单和统一的事情。</p>
</blockquote>

<h4>Log Files and Events</h4>

<p>Let&#39;s talk a little bit about a side benefit of this architecture: it enables decoupled, event-driven systems.</p>

<blockquote>
<p>采用中央日志管道，还有一个架构上的好处，构建了一个解耦，事件驱动的系统。这节作者主要通过LinkedIn内部的例子来说明使用中央日志管道解耦架构的好处。解耦一个系统当然有很多好处，包括，降低各系统间的相互依赖，便于对独立系统做单独的优化和扩展，降低开发和维护成本等。有一定实践经验的读者肯定能有自己的体会。</p>
</blockquote>

<p>The typical approach to activity data in the web industry is to log it out to text files where it can be scrapped into a data warehouse or into Hadoop for aggregation and querying. The problem with this is the same as the problem with all batch ETL: it couples the data flow to the data warehouse&#39;s capabilities and processing schedule.</p>

<p>At LinkedIn, we have built our event data handling in a log-centric fashion. We are using Kafka as the central, multi-subscriber event log. We have defined several hundred event types, each capturing the unique attributes about a particular type of action. This covers everything from page views, ad impressions, and searches, to service invocations and application exceptions.</p>

<p>To understand the advantages of this, imagine a simple event—showing a job posting on the job page. The job page should contain only the logic required to display the job. However, in a fairly dynamic site, this could easily become larded up with additional logic unrelated to showing the job. For example let&#39;s say we need to integrate the following systems:</p>

<ol>
<li>We need to send this data to Hadoop and data warehouse for offline processing purposes</li>
<li>We need to count the view to ensure that the viewer is not attempting some kind of content scraping</li>
<li>We need to aggregate this view for display in the Job poster&#39;s analytics page</li>
<li>We need to record the view to ensure we properly impression cap any job recommendations for that user (we don&#39;t want to show the same thing over and over)</li>
<li>Our recommendation system may need to record the view to correctly track the popularity of that job</li>
<li>Etc</li>
</ol>

<p>Pretty soon, the simple act of displaying a job has become quite complex. And as we add other places where jobs are displayed—mobile applications, and so on—this logic must be carried over and the complexity increases. Worse, the systems that we need to interface with are now somewhat intertwined—the person working on displaying jobs needs to know about many other systems and features and make sure they are integrated properly. This is just a toy version of the problem, any real application would be more, not less, complex.</p>

<p>The &quot;event-driven&quot; style provides an approach to simplifying this. The job display page now just shows a job and records the fact that a job was shown along with the relevant attributes of the job, the viewer, and any other useful facts about the display of the job. Each of the other interested systems—the recommendation system, the security system, the job poster analytics system, and the data warehouse—all just subscribe to the feed and do their processing. The display code need not be aware of these other systems, and needn&#39;t be changed if a new data consumer is added.</p>

<h4>Building a Scalable Log</h4>

<p>Of course, separating publishers from subscribers is nothing new. But if you want to keep a commit log that acts as a multi-subscriber real-time journal of everything happening on a consumer-scale website, scalability will be a primary challenge. Using a log as a universal integration mechanism is never going to be more than an elegant fantasy if we can&#39;t build a log that is fast, cheap, and scalable enough to make this practical at scale.</p>

<p>Systems people typically think of a distributed log as a slow, heavy-weight abstraction (and usually associate it only with the kind of &quot;metadata&quot; uses for which Zookeeper might be appropriate). But with a thoughtful implementation focused on journaling large data streams, this need not be true. At LinkedIn we are currently running over 60 billion unique message writes through Kafka per day (several hundred billion if you count the writes from mirroring between datacenters).</p>

<blockquote>
<p>LinkedIn的Kafka每天吞吐600亿条消息。</p>
</blockquote>

<p>We used a few tricks in Kafka to support this kind of scale:</p>

<ol>
<li>Partitioning the log 日志分区</li>
<li>Optimizing throughput by batching reads and writes 利用批量读写来优化吞吐</li>
<li>Avoiding needless data copies 避免不必要的数据备份</li>
</ol>

<p>In order to allow horizontal scaling we chop up our log into partitions:</p>

<p><img src="/generated/2014-01-22/log_partitions-350x205-0e2a99.png" alt="log_partitions.png" ></p>

<p>Each partition is a totally ordered log, but there is no global ordering between partitions (other than perhaps some wall-clock time you might include in your messages). The assignment of the messages to a particular partition is controllable by the writer, with most users choosing to partition by some kind of key (e.g. user id). Partitioning allows log appends to occur without co-ordination between shards and allows the throughput of the system to scale linearly with the Kafka cluster size.</p>

<blockquote>
<p>每个分区的日志都是有序的，消息被分配到哪个分区是受writer控制的，划分依据是预先定义好的字段。分区使得各分块的append操作不需要相互协调，并使得kafka的吞吐量随着集群规模可以线性扩展。</p>
</blockquote>

<p>Each partition is replicated across a configurable number of replicas, each of which has an identical copy of the partition&#39;s log. At any time, a single one of them will act as the leader; if the leader fails, one of the replicas will take over as leader.</p>

<blockquote>
<p>每个分区都有自己的副本，当主副本失效的时候，其他副本会取代主</p>
</blockquote>

<p>Lack of a global order across partitions is a limitation, but we have not found it to be a major one. Indeed, interaction with the log typically comes from hundreds or thousands of distinct processes so it is not meaningful to talk about a total order over their behavior. Instead, the guarantees that we provide are that each partition is order preserving, and Kafka guarantees that appends to a particular partition from a single sender will be delivered in the order they are sent.</p>

<blockquote>
<p>没有全局的序是一个问题，但并不是一个大问题。由于来源的数据源众多，所以强调全局的序没有太大的实际意义。每个partition内部是保证有序的。</p>
</blockquote>

<p>A log, like a filesystem, is easy to optimize for linear read and write patterns. The log can group small reads and writes together into larger, high-throughput operations. Kafka pursues this optimization aggressively. Batching occurs from client to server when sending data, in writes to disk, in replication between servers, in data transfer to consumers, and in acknowledging committed data.</p>

<blockquote>
<p>使用批量读写的方式优化吞吐量</p>
</blockquote>

<p>Finally, Kafka uses a simple binary format that is maintained between in-memory log, on-disk log, and in network data transfers. This allows us to make use of numerous optimizations including zero-copy data transfer.</p>

<blockquote>
<p>使用简单的二进制格式组织数据</p>
</blockquote>

<p>The cumulative effect of these optimizations is that you can usually write and read data at the rate supported by the disk or network, even while maintaining data sets that vastly exceed memory.</p>

<p>This write-up isn&#39;t meant to be primarily about Kafka so I won&#39;t go into further details. You can read a more detailed overview of LinkedIn&#39;s approach <a href="http://sites.computer.org/debull/A12june/pipeline.pdf">here</a> and a thorough overview of Kafka&#39;s design <a href="http://kafka.apache.org/documentation.html#design">here</a>.</p>

</div>

<div id="disqus_thread"></div>
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'creatstar'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>


          <div class="footer">
            <div class="contact">
              <p>
                我们的微博<br/>
              </p>
            </div>
            <div class="contact">
              <p>
                <a href="http://weibo.com/creatstar">叶谦creatstar</a>
                <a href="http://weibo.com/u/1800945442">姝婧Winnie伪班客</a><br/>
              </p>
            </div>
            <div class="contact">
              <p>
                版权声明：本站所有内容，未经特别说明，均采用<a href="http://creativecommons.org/licenses/by-nc-nd/3.0/deed.zh">“署名-非商业性使用-禁止演绎 3.0”协议</a>授权。任何违反本协议的行为均属于非法行为。如需非商业性转载，请保留署名。如需商业性转载出版，请直接和我们联系。
              </p>
            </div>
          </div>

        </div>
    </body>
</html>
